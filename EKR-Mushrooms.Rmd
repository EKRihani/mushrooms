---
title: "Mushrooms Dataset Analysis"
author: "E.K. RIHANI"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    number_sections: true   # Add section numbering
    extra_dependencies: ["float"]
    toc: true
    toc_depth: 3

header-includes:
indent: true   # Indent paragraphs
bibliography: packages.bib #, references.bib
references:
  - id: mushroom2
    title: "Mushroom data creation, curation, and simulation to support classification tasks"
    author: D.Wagner, D.Heider, G.Hattab
    url: https://doi.org/10.1038/s41598-021-87602-3
    DOI: 10.1038/s41598-021-87602-3
    publisher: Scientific Reports
    type: article-journal
    issued:
      year: 2021
  - id: mushroom2b
    title: "Mushrooms & Toadstools"
    author: P.Hardin
    url: https://doi.org/10.1038/s41598-021-87602-3
    DOI: 10.1038/s41598-021-87602-3
    publisher: Collins
    type: book
    issued:
      year: 2012
  - id: mushroom1
    title: "Mushroom Data Set"
    author: J.Schlimmer
    url: https://archive.ics.uci.edu/ml/datasets/Mushroom
    DOI: 
    publisher: University of California
    type: article-journal
    issued:
      year: 1987
  - id: mushroom1b
    title: "The Audubon Society Field Guide to North American Mushrooms"
    author: G. H. Lincoff
    url: 
    DOI: 
    publisher: Alfred A. Knopf
    type: book
    issued:
      year: 1987
  - id: courtecuisse
    title: "Mushrooms and Toadstools"
    author: R.Courtecuisse, B. Duhem
    url: 
    DOI: 
    publisher: Collins
    type: book
    issued:
      year: 1995

csl: https://www.zotero.org/styles/vancouver-superscript
#- \usepackage{indentfirst} # Indent first line of each paragraph
---

```{r setup, include = FALSE, warning=FALSE}
load("EKR-mushrooms.RData")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
#library(ggplot2)
library(ggpubr)   # Combine plots (ggarrange)
library(rmarkdown)
library(knitr)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.pos = "!H")
knitr::write_bib(c("tidyverse", "rmarkdown", "knitr", "ggpubr"), "packages.bib")
#knitr::write_bib(c("tidyverse", "caret", "data.table", "reshape2", "ggplot2", "recommenderlab", "stringr", "ggpubr", "rmarkdown", "knitr"), "packages.bib")
```
\newpage
\section{Overview}
\subsection{A word of caution}
\paragraph*{}
Mushroom identification is a difficult subject, that must **not** be taken lightly. The species of mushrooms one may encounter vary considerably from one ecosystem to another, from one continent to another, and no mushroom dataset or book can cover all the diversity of the fungal world.
\paragraph*{}
The dataset presented in this study, while being one of the most comprehensive mushrooms datasets in the data science area, is by no means comprehensive. 
\paragraph*{}
Several genera are not present in this dataset, the most famous one probably being the *psylocybe* genre, known for its psychedelic properties. Several species are only mentioned by their common names, such as *destroying angel*, which may be used for several different species : *Amanita virosa*, *Amanita verna*, *Amanita bisporigera*, *Amanita ocreata*... Several famous species are missing, some of them toxic (*Rubroboletus satanas* or Satan's bolete), some of them delicious (*Amanita caesaria* or Caesar's mushroom) and easily confused with toxic ones (*Amanita muscaria* or fly agaric, which may lose its white spots, especially after strong rains), or the other way around (the entire *gyromitra* genre is missing, and can be confused with morels). 
\paragraph*{}
Some criteria may also vary wildly accordingly to the mushroom stage of maturity : while the green hats of mature *Amanita phalloides* (death caps) are easily recognizable, young specimens are white and can easily be confused with edible ones (such as specimens of the *Agaricus* genre).
\paragraph*{}
Some of these mushrooms are *deadly*, even when eaten in very small amounts. Mushroom poisoning diagnosis can be quite difficult, and often made to late to really act accordingly. Toxic compounds such as amanitins cannot be altered or destroyed by cooking or freezing, and will be absorbed by the intestine, pass into the bloodstream to then be filtered by the liver, destroying the liver cells, and then being excreted in the intestines, to be reabsorded, filtered again... each pass destroying the liver cells that survived the previous one, in a cycle known as hepato-enteric reabsorption.
\paragraph*{}
Do not, *under any circumstances*, use this kind of dataset to determine if a mushroom is edible or not.

\subsection{Goal of the study}
\paragraph*{}
Mushroom and plants identification is a classic classification problem, which is usually performed manually, using identification tables. Most of these identification tables are actually based on a tree-based process, which seems pretty logical, given the tree-based logic of evolutionary processes. However, this logic has several limits.
\paragraph*{}
The first limit is the number of missing links. Some species have obviously become extinct, which means some of the branches and nodes of the phylogenetic tree are missing, which can complicate the analysis when two related species have a lot of missing common branches and nodes. Some similarities between species can also be missed.
\paragraph*{}
The second, and deeper limit is the inherent logic of the evolutionary process. Two antagonist phenomenons are at stake : evolutionary divergence and convergence. Both phenomenons are linked to the necessary adaptation of species to their environment. The evolutionary divergence explains the diversity of mammals : bats, whales and horses are related, but look very different because they have adapted to very different environments. The evolutionary convergence explains the similarity between the wing of a bat and the wing of the bee. However, despite their apparent dissimilarity, the wing of the bat is closer to the human hand or the whale's fin than it is from the wing of the bee. The most reliable way to assess the evolutionary process and accurately find links between species is to analyze their genomes : visible characteristics can sometimes be very misleading. However, these characteristics often are the only ones available.
\paragraph*{}
The third problem is the primary endpoint of the classification. This endpoint may or may not be linked to the evolutionary process or the visible criteria, especially if this endpoint is vague. The endpoint of this study (edibility) lies in this category, for several reasons : it is mostly centered on human-only toxicity, many different toxicity mechanisms can exist, and the toxicity or non-toxicity of a fungic or vegetal metabolite can be linked to very tenuous metabolic modifications from one species to another.
\paragraph*{}
For these reasons among others, the tree-based logic, while commonly used in mushrooms and plants identification, and allegedly justified by the tree-based evolutionary process, may not necessarily be the most optimal approach to species classification based on visual criteria. The aim of this study is to perform this classification task based on limited visual cues, and to assess the performance of different classification strategies.
\subsection{The dataset}
\paragraph*{}
The Secondary Mushroom Identification Dataset was submitted by Dennis Wagner in 2021[@mushroom2][@mushroom2b], and is inspired by the Mushroom Dataset created by Jeff Schlimmer in 1987[@mushroom1][@mushroom1b]. Both datasets aim to create a fictional collection of mushrooms, built by gathering characteristics about a selected number of mushroom species, then randomizing each individual mushroom quantitative variables along the species typical characteristics (i.e. cap diameter, stem  height, stem width).  
\paragraph*{}
This dataset contains the data from n = `r summary_number` mushrooms (rows). Each mushroom has `r nrow(structure_dataset)` attributes (columns) :

* *Class* : edible (e) or poisonous (p)
* *Cap diameter* : in centimeters
* *Cap shape* : bell (b), conical (c), convex (x), flat (f), sunken (s), spherical (p) or other (o)
* *Cap surface* : fibrous (i), grooves (g), scaly (y), smooth (s), shiny (h), leathery (l), silky (k), sticky (t), wrinkled (w), downy (d) or fleshy (e)
* *Cap color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o) or black (k)
* *Does bruise or bleed* : true (t) or false (f)
* *Gill attachment* : adnate (a), adnexed (x), decurrent (d), free (e), sinuate (s), pores (p), none (f), unknown (?)
* *Gill spacing* : close (c), distant (d), none (f)
* *Gill color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Stem height* : in centimeters
* *Stem width* : in millimeters
* *Stem root* : bulbous (b), swollen (s), club (c), cup (u), equal (e), rhizomorphs (z), rooted (r) or none (f)
* *Stem surface* : fibrous (i), grooves (g), scaly (y), smooth (s), shiny (h), leathery (l), silky (k), sticky (t), wrinkled (w), fleshy (e) or none (f)
* *Stem color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Veil type* : partial (p) or universal (u)
* *Veil color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Has ring* : true (t) or false (f)
* *Ring type* : cobwebby (c), evanescent (e), flaring (r), grooved (g), large (l), pendant (p), sheathing (s), zone (z), scaly (y), movable (m), none (f) or unknown (?)
* *Spore print color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Habitat* : grasses (g), leaves"(l), meadows (m), paths (p), heaths (h), urban (u), waste (w) or woods (d)
* *Season* : spring (s), summer (u), autumn (a) or winter (w)

\newpage
\section{Methods and Analysis}
\subsection{Computer and R configuration}
\paragraph*{}
The code and report were run on a Xubuntu Linux 20.04 LTS system, with R 3.6.3 and the following packages : tidyverse[@R-tidyverse] (v1.3.1), caret[@-caret] (v6.0-88), ggpubr[@R-ggpubr] (v0.4.0), rmarkdown[@R-rmarkdown] (v2.11), knitr[@R-knitr] (v1.34).
\paragraph*{}
The code was also tested on a Windows 7 x64 computer on R 4.1.0. It was not tested on any Mac OS system.
\subsection{Data preparation and cleaning}
\paragraph*{}
The dataset is directly downloaded from the UCI database in zip format and unzipped. The archive contains four files :

* *primary_data.csv* : the csv file with all data used for creating the base. One line for each of the 173 species.
* *primary_data_meta.txt* : metadata for the primary dataset.
* *secondary_data.csv* : the final dataset, made of all  `r summary_number` specimen.
* *secondary_data_meta.txt* : metadata for the secondary dataset.

\paragraph*{}
The file of interest (*secondary_data.csv*) is extracted using the *read.csv* function.
\paragraph*{}
The initial and final structures of the dataset are as following :
```{r out.width = "90%", echo = FALSE}
kable(structure_dataset, caption = "Dataset structure")
```
The import automatically converted all character variables to factors. The only structural modification was to modify some of the 2-level factors into logical variables.
\paragraph*{}
The primary and secondary *data_meta* files include all information to translate the one-letter variable codes into comprehensible words. However, two variable codes are missing.
\paragraph*{}
Firstly, the *stem.root = f* value isn't explained. Since most *f* values for the other categorical stem variables (*stem.surface*, *stem.color*) mean the mushroom has no stem and given the fact that all stem.root = f values are always associated with *stem.surface = f*, *stem.color = f*, *stem.height = 0* and *stem.width = 0*, one can safely assume that *stem.root = f* simply means there is no stem.
```{r out.width = "100%", echo = FALSE}

kable(uniques_missing_f, caption = "Unique values for stem.root = 'f'")
```
\paragraph*{}
Secondly, the *cap.surface = d* value isn't explained either. There was no workaround to lift this uncertainty except from performing some search : digging into the primary dataset, finding all the mushrooms that may have the value *cap.surface = d*, translating the common english name into the scientific (latin) ones and finding the relevant descriptions in mycology books.[@courtecuisse]
\paragraph*{}
This research showed all these mushrooms to have a velvety, or downy surface. A value d = downy was thus assumed.
\paragraph*{}
These issues underline why it is important to provide extensive documentation, well-defined variables and values in every datasets and to follow international standards (such as latin names). Fortunately, the author provides the primary dataset with the characteristics of all species; without that file, it would have been impossible to find what characteristic lied under the *cap.surface = d* value.
\paragraph*{}

\subsection{Training set analysis}
\paragraph*{}
The original dataset was first randomly split into a 90%-sized training/validation set, and a 10%-sized evaluation set that is not to be used until the final validation.
\paragraph*{}
All dimensional distributions seem to roughly follow a bell curve, with a long tail towards the higher values.A logarithmic transformation can show more clearly the shape of this tail.  

```{r, echo = FALSE, fig.height = 2.5, fig.cap = "Mushroom cap diameter, stem height and stem width distribution"}
plot(ggarrange(
   ncol = 3,
   study_distrib_cap.diameter + ggtitle("") + ylab("") + scale_y_continuous(labels = NULL),
   study_distrib_stem.height + ggtitle("") + ylab("") + scale_y_continuous(labels = NULL),
   study_distrib_stem.width + ggtitle("") + ylab("") + scale_y_continuous(labels = NULL)
   )
)
```

```{r, echo = FALSE, fig.height = 2.5, fig.cap = "Mushroom cap diameter, stem height and stem width distribution (log Y scale)"}
plot(ggarrange(
   ncol = 3,
   study_distrib_cap.diameter + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   study_distrib_stem.height + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   study_distrib_stem.width + ggtitle("") + ylab("") + scale_y_log10(labels = NULL)
   )
)
```

The cap diameter distribution looks like a bell curve with a long right tail but actually is bimodal, with a main mode toward 5 cm, and a much smaller secondary mode toward 50 cm. This size can look quite surprising, but after further investigation, it appears that some species such as *Polyporus squamosus* (dryad's saddle) can be very large, with some specimens that can weight up to 5 kg.[@courtecuisse]
\paragraph*{}
The stem height distribution also looks like a bell curve with a long right tail, a main mode around 5 cm, and a secondary mode at 0 cm. Again, this can look surprising, but some mushrooms have no stem, which could explain this height value.
\paragraph*{}
The stem width distribution looks like a bell curve with a long right tail, and a peak around 10-15 mm. In all three distributions, the right tail can probably be explained by the impossibility to have negative dimensional values.

\subsection{Single and dual criteria classifiers}  
\paragraph*{}
All further classification strategies will be elaborated, tuned and selected on one rule : the specificity on the edibility criterion must be equal to 1, false negatives are not tolerated. In layman's terms, a good algorithm may reject edible mushrooms but should **never** classify a poisonous one as edible.
\paragraph*{}
The first models one can build are simple classifiers that categorize the mushrooms as edible or poisonous, based on one or two criteria.
\paragraph*{}
Before training, validation and tuning of the algorithms, a training and validation strategy must be defined. Since these clasifiers are designed from scratch, and in order to keep the code simple, a basic train-validation split was chosen, with a 90/10 split ratio between the training and validation sets.
\paragraph*{}
Once the original dataset is split in training, validation and evaluation sets, one can have a look at the characteristics of the edible and toxic populations and draw conclusions without any risk of overfitting.  

```{r, echo = FALSE, fig.height = 6, fig.cap = "Mushroom characteristics, training set (log y scale)"}
plot(ggarrange(
   ncol = 3, nrow = 2, common.legend = TRUE,
   train_distrib_cap.diameter + ggtitle("Cap Diameter") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   train_distrib_stem.height + ggtitle("Stem Height") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   train_distrib_stem.width + ggtitle("Stem Width") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   train_distrib_habitat + ggtitle("Habitat") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL) + theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust=1)),
   train_distrib_spore.print.color + ggtitle("Spore Print Color") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL) + theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust=1)),
   train_distrib_veil.color + ggtitle("Veil Color") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL) + theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust = 1))
   )
)
```  

Differences in the distribution of edible and non edible species can already be seen, and these variations will be used to classify species.
\paragraph*{}
Given the specificity constraint mentioned previously, a "stupid" algorithm can be designed as a benchmark. This algorithm will simply classify all mushrooms as poisonous, by setting all edibility values to FALSE.
\paragraph*{}
A slightly more elaborate algorithm can classify mushrooms as edible if they meet one criterion. For example, in the distributions showed above, the mushroom should be edible if *cap.diameter* > 40, if *spore.print.color* = gray or if *veil.color* = yellow.
\paragraph*{}
A more sophisticated algorithm can be elaborated with two factors and will for example, classify a mushroom as edible if *cap.shape* = convex & *cap.surface* = leathery.

```{r, echo = FALSE, fig.height = 6, fig.cap = "Secondary mushroom dataset structure"}
pair_plots
```  

The building of this more performant model will be discussed here, since the single-criterion one is very similar and can be seen as a simplified version of the two-criteria model.
\paragraph*{}
The very first step was to build the criteria list. All unique *factor* and *logical*  types variables were gathered (with *level* value). The same operation was performed on *numeric* type values, which were then duplicated for *min* and *max* levels. The two datalists were then merged, and joined with a datalist that contained all *type* characteristics of each variable.
\paragraph*{}
The building of a comprehensive -- but not redundant -- 2-criteria list is a four step process :

1. Create a 2-index list with the *ncomb* function, which eliminates potential duplicates because of the commutative properties of the AND operator, i.e. (n,p) indices are equivalent to (p,n) ;
2. Take each column to build the corresponding half-sized dataset and merge the two halves ;
3. Remove all single-criterion combinations, i.e all (n,n) indices ;
4. Set the edibility criterion to FALSE by default.
\paragraph*{}
All criteria combinations are then evaluated. The first step of this evaluation is to detect the characteristic type. All mushroom characteristics fall into one of two *lato sensu* categories : "text" (i.e. character, factor or logical) or "numbers" (i.e. integer or numeric). With a two-criteria classifier, these two types should create four combinations :

* Criterion 1 is text, criterion 2 is numeric ;
* Criterion 1 is text, criterion 2 is text ;
* Criterion 1 is numeric, criterion 2 is numeric ;
* Criterion 1 is numeric, criterion 2 is text.

However, by construction, the fourth combination does not exist, which somewhat simplifies the evaluation function. This is a consequence of the use of *ncomb* function and the placement of all numeric factors were placed at the bottom of the criterion list, and this property can be illustrated by :
```{r}
factors_list2 %>% 
  filter(type2 %in% c("logical", "factor", "character"), 
         type1 %in% c("integer", "numeric")) %>% 
  nrow
```  
\paragraph*{}
After the type identification and a filtering step on the two relevant criterias, the basic classification strategy is as follows :

* Text + text : count all occurrences, count all poisonous occurrences. If there are mushrooms, but none is poisonous, consider all of them as edible.
* Text + numeric : compute min (or max, resp.) on all occurrences, and on all poisonous occurrences. If the values differ, consider all values below (or above, resp.) the poisonous occurences to be all edible. Apply a settable safety margin.
* Numeric + numeric : compute the two min (or max, resp.) on each criteria, on all occurrences and on all poisonous occurences. If the values differ on each dimension, consider all values below (or above, resp.) the poisonous occurences to be all edible. Apply a settable safety margin on each dimension.

The min/max detection is very simply based on the "min" or "max" string found in *factors_listX$typeY*. A *match.fun* function converts the "min" or "max" string into a *min* or *max* function, a *recode.factor* function was used to select the sign (+/-) of the margin. The "min" or "max" string was then concatenated to the computed numeric value, and converted into a > or < symbol, to be used in the evaluation step.
\paragraph*{}

```{r out.width = "100%", echo = FALSE}

kable(head(relevant_factors2, 15), caption = "15 first relevant factors of the two-criteria classifier")
```

This list will serve as a basis to create a criterion list that merges all criteria two by two (for a two-criteria model) with AND operators, then merges all the pairs together with OR operators. The resulting string is simply evaluated against the validation set with a mutate function : if one criteria is met, the predicted edibility is set to TRUE, if not, it is set to FALSE.
\paragraph*{}
The predicted edibility is then compared to the real edibility value.
\paragraph*{}
By construction, the stupid classifier has *Sensitivity =* `r CM_stupid$byClass["Sensitivity"]` and *Specificity = * `r CM_stupid$byClass["Specificity"]`. The relatively good accuracy score (*Accuracy =* `r round(CM_stupid$overall["Accuracy"], 3)`) is indeed misleading, this model actually performs extremely poorly, with a F1 value that cannot be computed by the *confusionMatrix* function, since this model didn't classify correctly any of the edible mushrooms.
```{r out.width = "100%", echo = FALSE}

kable(CM_stupid$table, caption = "Confusion Matrix for stupid classifier")
```

\paragraph*{}
The single-criterion classifier performance was only marginally better than the stupid classifier's, and achieved *Sens. =* `r round(CM_monocrit$byClass["Sensitivity"], 3)` and *Spec. =* `r round(CM_monocrit$byClass["Specificity"], 3)` for a safety margin set at $m_{2} =$ `r margin1`. The F1 value was poor : *F1 =* `r round(CM_monocrit$byClass["F1"] , 3)`. Of the `r sum(CM_monocrit$table[,2])` edible specimen in the validation set, only `r sum(CM_monocrit$table[2,2])` were recognized as such.
```{r out.width = "100%", echo = FALSE}

kable(CM_monocrit$table, caption = "Confusion Matrix for single-criterion classifier")
```
\paragraph*{}
Given the poor performance of the single-criterion classifier, the performance of the dual-criteria classifier were quite surprising. With a safety margin set at $m_{2} =$ `r margin2`, this model managed to attain *Sens. =* `r round(CM_bicrit$byClass["Sensitivity"], 3)`, 
*Spec. =* `r round(CM_bicrit$byClass["Specificity"], 3)`, and 
*F1 =* `r round(CM_bicrit$byClass["F1"],3)`. Of the `r sum(CM_bicrit$table[,2])` edible specimen of the validation set, `r CM_bicrit$table[2,2]` were identified correctly.

```{r out.width = "100%", echo = FALSE}

kable(CM_bicrit$table, caption = "Confusion Matrix for dual-criteria classifier")
```
\paragraph*{}
The single-criterion and dual-criteria safety margins ($m_{1}$ and $m_{2}$) are two hyperparameters that can be tuned to give optimal results. A margin range was built, using the *seq* function, and then fed to our evaluation function through a *sapply* function.
\paragraph*{}
As said in the previous section, the endpoint is not an optimum on the ROC curve, but rather the margin that obeys to the following criteria, in decreasing priority order 

1. Specificity equal to 1,
2. Highest possible sensitivity,
3. Largest (*i.e.* safest) margin.  
\paragraph*{}
On the single-criterion model, the hyperparameter optimization gave the following results :  

```{r, echo = FALSE, fig.height = 3, fig.cap = "Sensitivity, specificity and F1-score of single-criteron classifier"}
plot(ggarrange(
   ncol = 3,
   SCtuneSensitivity,
   SCtuneSpecificity,
   SCtuneF1
   )
)
```  

An optimum was reached for $m_{1} =$ `r best_margin1["Margin"]`, which gives $Sens_{1} =$ `r best_margin1["Sensitivity"]` for $Spec_{1}$ = `r best_margin1["Specificity"]`.
\paragraph*{}
A second hyperparameter optimization run was performed on the two-criteria model, using the same $m_{1}$ value for all single-criterion classification. This run gave the following sensitivity, specificity and F1-score plots :  

```{r, echo = FALSE, fig.height = 3, fig.cap = "Sensitivity, specificity and F1-score of dual-criteron classifier"}
plot(ggarrange(
   ncol = 3,
   DCtuneSensitivity,
   DCtuneSpecificity,
   DCtuneF1
   )
)
```  

The best result was achieved with $m_{1} =$ `r best_margin1["Margin"]` and $m_{2} =$ `r best_margin2["Margin"]` , which a final performance of $Sens_{2} =$ `r best_margin2["Sensitivity"]` for $Spec_{2} =$ `r best_margin2["Specificity"]`. The specificity constraint does not provide the best F1-score.

\newpage
\subsection{Caret package models}  
The *caret* packages provides a very convenient and efficient platform for data modelling and inference. This section will explain the strategy used for the evaluation of some of these models. The selected models were of various types :  
* Linear Discriminant Analysis : Linear Discriminant Analysis (lda2), Penalized Discriminant Analysis (pda)
* Generalized Additive Model : Generalized Additive Model using LOESS (gamLoess)
* Tree-Based Models : Classification And Regression Tree (CART) (rpart, rpartCost), Conditional Inference Tree (ctree), Single C5.0 Tree (ctree)
* Random Forest : Random Ferns (rferns), Random Forest (ranger, Rborist)

\paragraph*{}
The first step was to build a regression and evaluation function. The caret package allows to define it very simply with :

```
set.seed(1)
tr_ctrl <- trainControl(classProbs = TRUE, 
                        summaryFunction = twoClassSummary, 
                        method = "cv", number = 10)
train(class ~ ., 
      method = [METHOD], 
      data = trainvalid_set, 
      trControl = tr_ctrl, 
      metric = 'Spec', 
      tuneGrid = data.frame([PARAMETERS]))
```
The *set.seed* insures reproducibility, the *trainControl* function allows to control various training and evaluation parameters ; in this case, to use a ROC/sensitivity/specificity criterion and a 10-fold cross-validation. The *train* function runs the train and evaluation process, while allowing the use of several parameters, such as the method selection the training/validation set, the metric used for the evaluation and the grid of model parameters.
\paragraph*{}
This block was included in a function to allow easy access and reproducibility. The function returns a list that can be plotted and also includes various data of interest, such as .\$results (ROC, Sensibility, Specificity), .\$bestTune (best value with the evaluation metric, Specificity in this case), and .\$finalModel (miscellanous information that can sometimes be plotted, such as trees).

\subsubsection{Linear Discriminant Analysis Models}  
The two Linear Discriminant Analysis Models selected for this study are lda2 (Linear Discriminant Analysis) and pda (Penalized Discriminant Analysis). The lda2 model has one tuning parameter (*dimen*, number of discriminant functions). The pda model also has a single tuning parameter (*lambda*, shrinkage penalty coefficient).
```{r, echo = FALSE, fig.height = 3, fig.cap = "Specificity of lda2 (left) and pda (right) models"}
plot(ggarrange(
   ncol = 2,
   ggplot(fit_lda2_dim) + theme_bw(),
   ggplot(fit_pda_lambda) + theme_bw()
   )
)
```  
\paragraph*{}
The *dimen* parameter of the lda2 model does not seem to have much effect on specificity (*Spec =* `r round(mean(fit_lda2_dim$results["Spec"]))`). 
\paragraph*{}
The *lambda* parameter of the sda model marginally affects its specificity, with lower lambda values giving sligthly better results ($Spec_max =$ `r round(max(fit_pda_lambda$results["Spec"]))`).
\paragraph*{}
However, the specificity of both models are not sufficient for this study.

\subsubsection{Generalized Additive Model}  

A tester puis faire.

\subsubsection{Tree-Based Models}
\paragraph*{}
Tree-based models are of special interest in this study, for two main reasons :
* Tree-based logic is usually used in manual mushrooms classification,
* Tree models can be plotted and easily understood by humans.

\paragraph*{}
The first models presented in this study are two CART (Classification And Regression Tree) models. The basic CART model (rpart) has one complexity parameter (cp).
```{r, echo = FALSE}

kable(round(fit_rpart_cp$results[,1:4],5), caption = "Performance of the CART (rpart) model")
```

The basic CART model does never achieve the required specificity. However, this model comes close and the tree structure can give some hints about the most important criteria.

```{r, echo = FALSE, fig.cap = "Decision tree for the CART model (cp = 0.02)"}
plot(fit_rpart_example$finalModel, margin = 0.1)
text(fit_rpart_example$finalModel, cex = 0.6)
``` 


\subsubsection{Random Forest Models}

\newpage
\section{Results}
\subsection{Evaluation protocol}  
\paragraph*{}
All models were trained on the training dataset, set with the best hyperparameters values obtained by evaluating the performance against the validation dataset. Their performance against the evaluation dataset will be analyzed, using the same criteria as before :

1. Specificity *must* be equal to one.
2. Sensitivity should be the highest possible.

\subsection{Dual criteria classifier performance}  
\paragraph*{}
Running the dual criteria classifier against the evaluation dataset yields the following results :
```{r out.width = "100%", echo = FALSE}
kable(results_biclass, caption = "Performance of the bi-criteria classifier against the evaluation dataset")
```
\paragraph*{}
The confusion matrix shows more accurately the results : while `r CM_bifinal$table[2,2]` of the `r sum(CM_bifinal$table[,2])` edible specimen were accurately identified, `r CM_bifinal$table[2,1]` non-edible specimen were incorrectly classified as edible.  
```{r out.width = "100%", echo = FALSE}
kable(CM_bifinal$table, caption = "Confusion Matrix of the bi-criteria classifier against the evaluation dataset")
```  

While the model performance is honorable, it is not sufficient to fulfill the specificity criterion, which was the primary endpoint of this study.  


\subsection{Caret package models}  


*Section that presents the modeling results and discusses the model performance.*

\newpage
\section{Conclusion}

*Section that gives a brief summary of the report, its limitations and future work*

\newpage
\section{References}
