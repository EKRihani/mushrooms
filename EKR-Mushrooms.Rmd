---
title: "Mushrooms Dataset Analysis"
author: "E.K. RIHANI"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    number_sections: true
    extra_dependencies: ["float"]
    toc: true
    toc_depth: 3

header-includes:
indent: true
bibliography: packages.bib
references:
  - id: mushroom2
    title: "Mushroom data creation, curation, and simulation to support classification tasks"
    author: D.Wagner, D.Heider, G.Hattab
    url: https://doi.org/10.1038/s41598-021-87602-3
    DOI: 10.1038/s41598-021-87602-3
    publisher: Scientific Reports
    type: article-journal
    issued:
      year: 2021
  - id: mushroom2b
    title: "Mushrooms & Toadstools"
    author: P.Harding
    url: https://doi.org/10.1038/s41598-021-87602-3
    DOI: 10.1038/s41598-021-87602-3
    publisher: Collins
    type: book
    issued:
      year: 2012
  - id: mushroom1
    title: "Mushroom Data Set"
    author: J.Schlimmer
    url: https://archive.ics.uci.edu/ml/datasets/Mushroom
    DOI: 
    publisher: University of California
    type: article-journal
    issued:
      year: 1987
  - id: mushroom1b
    title: "The Audubon Society Field Guide to North American Mushrooms"
    author: G. H. Lincoff
    url: 
    DOI: 
    publisher: Alfred A. Knopf
    type: book
    issued:
      year: 1987
  - id: courtecuisse
    title: "Mushrooms and Toadstools"
    author: R.Courtecuisse, B. Duhem
    url: 
    DOI: 
    publisher: Collins
    type: book
    issued:
      year: 1995

csl: https://www.zotero.org/styles/vancouver-superscript
#- \usepackage{indentfirst} # Indent first line of each paragraph
---

```{r setup, include = FALSE, warning=FALSE}
load("EKR-mushrooms.RData")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
#library(ggplot2)
library(ggpubr)   # Combine plots (ggarrange)
library(rmarkdown)
library(knitr)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.pos = "!H")
knitr::write_bib(c("tidyverse", "caret",  "rmarkdown", "GGally", "plyr", "MASS", "mda", "rpart", "C50", "party", "ranger", "Rborist", "e1071", "rFerns", "knitr", "ggpubr"), "packages.bib")
```
\newpage
\section{Overview}
\subsection{A word of caution}
\paragraph*{}
Mushroom identification is a difficult subject, that must **not** be taken lightly. The species of mushrooms one may encounter vary considerably from one ecosystem to another, from one continent to another, and no mushroom dataset or book can cover all the diversity of the fungal world.
\paragraph*{}
The dataset presented in this study, while being one of the most comprehensive mushrooms datasets in the data science area, is by no means comprehensive. 
\paragraph*{}
Several genera are not present in this dataset, the most famous one probably being the *psylocybe* genre, known for its psychedelic properties. Several species are only mentioned by their common names, such as *destroying angel*, which may be used for several different species : *Amanita virosa*, *Amanita verna*, *Amanita bisporigera*, *Amanita ocreata*... Several famous species are missing, some of them toxic (*Rubroboletus satanas* or Satan's bolete), some of them delicious (*Amanita caesaria* or Caesar's mushroom) and easily confused with toxic ones (*Amanita muscaria* or fly agaric, which may lose its white spots, especially after strong rains), or the other way around (the entire *gyromitra* genre is missing, and can be confused with morels). 
\paragraph*{}
Some criteria may also vary wildly accordingly to the mushroom stage of maturity : while the green hats of mature *Amanita phalloides* (death caps) are easily recognizable, young specimens are white and can easily be confused with edible ones (such as specimens of the *Agaricus* genre).
\paragraph*{}
Some of these mushrooms are *deadly*, even when eaten in very small amounts. Mushroom poisoning diagnosis can be quite difficult, and often made to late too really act accordingly. Toxic compounds such as amanitins cannot be altered or destroyed by cooking or freezing, and will be absorbed by the intestine, pass into the bloodstream to then be filtered by the liver, destroying the liver cells, and then being excreted in the intestines, to be reabsorded, filtered again... each pass destroying the liver cells that survived the previous one, in a cycle known as hepato-enteric reabsorption.
\paragraph*{}
Do not, *under any circumstances*, use this kind of dataset to determine if a mushroom is edible or not.

\subsection{Goal of the study}
\paragraph*{}
Mushroom and plants identification is a classic classification problem, which is usually performed manually, using identification tables. Most of these identification tables are actually based on a tree-based process, which seems pretty logical, given the tree-based logic of evolutionary processes. However, this logic has several limits.
\paragraph*{}
The first limit is the number of missing links. Some species have obviously become extinct, which means some of the branches and nodes of the phylogenetic tree are missing, which can complicate the analysis when two related species have a lot of missing common branches and nodes. Some similarities between species can also be missed.
\paragraph*{}
The second, and deeper limit is the inherent logic of the evolutionary process. Two antagonist phenomenons are at stake : evolutionary divergence and convergence. Both phenomenons are linked to the necessary adaptation of species to their environment. Evolutionary divergence explains the diversity of mammals : bats, whales and horses are related, but look very different because they have adapted to very different environments. On the other hand, evolutionary convergence explains the similarity between the wing of a bat and the wing of the bee. However, despite their apparent dissimilarity, the wing of the bat is closer to the human hand or the whale's fin than it is from the wing of the bee. The most reliable way to assess the evolutionary process and accurately find links between species is to analyze their genomes : visible characteristics can sometimes be very misleading. However, these characteristics often are the only ones available.
\paragraph*{}
The third problem is the primary endpoint of the classification. This endpoint may or may not be linked to the evolutionary process or the visible criteria, especially if this endpoint is vague. The endpoint of this study (edibility) lies in this category, for several reasons : it is mostly centered on human-only toxicity, many different toxicity mechanisms can exist, and the toxicity or non-toxicity of a fungic or vegetal metabolite can be linked to very tenuous metabolic modifications from one species to another.
\paragraph*{}
For these reasons among others, the tree-based logic, while commonly used in mushrooms and plants identification, and allegedly justified by the tree-based evolutionary process, may not necessarily be the most optimal approach to species classification based on visual criteria. The aim of this study is to perform this classification task based on limited visual cues, and to assess the performance of different classification strategies.
\subsection{The dataset}
\paragraph*{}
The Secondary Mushroom Identification Dataset was submitted by Dennis Wagner in 2021[@mushroom2][@mushroom2b], and is inspired by the Mushroom Dataset created by Jeff Schlimmer in 1987[@mushroom1][@mushroom1b]. Both datasets aim to create a fictional collection of mushrooms, built by gathering characteristics about a selected number of mushroom species, then randomizing each individual mushroom quantitative variables along the species typical characteristics (i.e. cap diameter, stem  height, stem width).  
\paragraph*{}
This dataset contains the data from n = `r summary_number` mushrooms (rows). Each mushroom has `r nrow(structure_dataset)` attributes (columns) :

* *Class* : edible (e) or poisonous (p)
* *Cap diameter* : in centimeters
* *Cap shape* : bell (b), conical (c), convex (x), flat (f), sunken (s), spherical (p) or other (o)
* *Cap surface* : fibrous (i), grooves (g), scaly (y), smooth (s), shiny (h), leathery (l), silky (k), sticky (t), wrinkled (w), downy (d) or fleshy (e)
* *Cap color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o) or black (k)
* *Does bruise or bleed* : true (t) or false (f)
* *Gill attachment* : adnate (a), adnexed (x), decurrent (d), free (e), sinuate (s), pores (p), none (f), unknown (?)
* *Gill spacing* : close (c), distant (d), none (f)
* *Gill color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Stem height* : in centimeters
* *Stem width* : in millimeters
* *Stem root* : bulbous (b), swollen (s), club (c), cup (u), equal (e), rhizomorphs (z), rooted (r) or none (f)
* *Stem surface* : fibrous (i), grooves (g), scaly (y), smooth (s), shiny (h), leathery (l), silky (k), sticky (t), wrinkled (w), fleshy (e) or none (f)
* *Stem color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Veil type* : partial (p) or universal (u)
* *Veil color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Has ring* : true (t) or false (f)
* *Ring type* : cobwebby (c), evanescent (e), flaring (r), grooved (g), large (l), pendant (p), sheathing (s), zone (z), scaly (y), movable (m), none (f) or unknown (?)
* *Spore print color* : brown (n), buff (b), gray (g), green (r), pink (p), purple (u), red (e), white (w), yellow (y), blue (l), orange (o), black (k) or none (f)
* *Habitat* : grasses (g), leaves"(l), meadows (m), paths (p), heaths (h), urban (u), waste (w) or woods (d)
* *Season* : spring (s), summer (u), autumn (a) or winter (w)

\newpage
\section{Methods and Analysis}
\subsection{Computer and R configuration}
\paragraph*{}
The code, final benchmarks and report were run on the following computer configuration :

* CPU : AMD Ryzen 5 5600G
* RAM : 2x16 GB DDR4-3200
* SSD : Crucial P5 M2 NVMe
* OS : Xubuntu Linux 20.04 LTS
* R : version 3.6.3
* Packages : tidyverse[@R-tidyverse] (v1.3.1), caret[@R-caret] (v6.0-88), GGally[@R-GGally] (v2.1.2), MASS[@R-MASS] (v7.3-51.5), mda[@R-mda] (v0.4-10), rpart[@R-rpart] (v4.1-15), plyr[@R-plyr] (v1.8.5), C50[@R-C50] (v0.1.5), party[@R-party] (v1.3-9), ranger[@R-ranger] (v0.13.1), e1071[@R-e1071] (v1.7-9), rFerns[@R-rFerns] (v5.0.0), Rborist[@R-Rborist] (v0.2-3), ggpubr[@R-ggpubr] (v0.4.0), rmarkdown[@R-rmarkdown] (v2.11), knitr[@R-knitr] (v1.34).

\paragraph*{}
The code was also tested on a Windows 7 x64 computer with R 4.1.0, and on a laptop with 4GB RAM. 
\paragraph*{}
It was not tested on any Mac OS system.


\subsection{Data preparation and cleaning}
\paragraph*{}
The dataset is directly downloaded from the UCI database in zip format and unzipped. The archive contains four files :

* *primary_data.csv* : the csv file with all data used for creating the base. One line for each of the 173 species.
* *primary_data_meta.txt* : metadata for the primary dataset.
* *secondary_data.csv* : the final dataset, made of all `r summary_number` specimen.
* *secondary_data_meta.txt* : metadata for the secondary dataset.

\paragraph*{}
The file of interest (*secondary_data.csv*) is extracted using the *read.csv* function.

\paragraph*{}
\newpage
The initial and final structures of the dataset are as following :
```{r out.width = "90%", echo = FALSE}
kable(structure_dataset, caption = "Dataset structure")
```
The importation automatically attributed character and numeric type to imported data.
\paragraph*{}
The primary and secondary *data_meta* files include all information to translate the one-letter variable codes into comprehensible words. This step was performed with the *recode_factor* function, which also automatically shifted all character type variables to the factor type. The only remaining structural modification was to modify some of the 2-level factors into logical variables. 
\paragraph*{}
However, two variable codes were still missing.
\paragraph*{}
Firstly, the *stem.root = f* value wasn't explained. Since most *f* values for the other categorical variables linked to stem properties (*stem.surface*, *stem.color*) meant the mushroom has no stem, and given the fact that all *stem.root = f* values are always associated with *stem.surface = f*, *stem.color = f*, *stem.height = 0* and *stem.width = 0*, one can safely assume that *stem.root = f* simply means the mushroom has no stem.
```{r out.width = "100%", echo = FALSE}

kable(uniques_missing_f, caption = "Unique values for stem.root = 'f'")
```
\paragraph*{}
Secondly, the *cap.surface = d* value wasn't explained either. There was no workaround to lift this uncertainty except from performing some search : digging into the primary dataset, finding all the species that may have the value *cap.surface = d*, translating the common english name into the scientific (latin) ones and finding the relevant descriptions in specialized mycology books.[@courtecuisse]
\paragraph*{}
This research showed all these mushrooms to have a velvety, or downy surface. A value d = downy was thus assumed.
\paragraph*{}
These issues underline the importance of providing extensive documentation, well-defined variables and values in every datasets and of following international standards (such as latin names). Fortunately, the author provides the primary dataset with the characteristics of all species ; without that file, it would have been impossible to find what characteristic lied under the *cap.surface = d* value.
\paragraph*{}

\subsection{Training set analysis}
\paragraph*{}
The original dataset was first randomly split into a 90%-sized training/validation set, and a 10%-sized evaluation set that is not to be used until the final validation.
\paragraph*{}
All dimensional distributions seem to roughly follow a bell curve (fig. 1), with a long tail towards the higher values. A logarithmic transformation (fig. 2) can show more clearly the shape of this tail.  

```{r, echo = FALSE, fig.height = 2.5, fig.cap = "Mushroom cap diameter, stem height and stem width distribution"}
plot(ggarrange(
   ncol = 3,
   study_distrib_cap.diameter + ggtitle("") + ylab("") + scale_y_continuous(labels = NULL),
   study_distrib_stem.height + ggtitle("") + ylab("") + scale_y_continuous(labels = NULL),
   study_distrib_stem.width + ggtitle("") + ylab("") + scale_y_continuous(labels = NULL)
   )
)
```

```{r, echo = FALSE, fig.height = 2.5, fig.cap = "Mushroom cap diameter, stem height and stem width distribution (log Y scale)"}
plot(ggarrange(
   ncol = 3,
   study_distrib_cap.diameter + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   study_distrib_stem.height + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   study_distrib_stem.width + ggtitle("") + ylab("") + scale_y_log10(labels = NULL)
   )
)
```

The cap diameter distribution looks like a bell curve with a long right tail but actually is bimodal, with a main mode toward 5 cm, and a much smaller secondary mode toward 50 cm. This size can look quite surprising, but after further investigation, it appears that some species such as *Polyporus squamosus* (dryad's saddle) can be very large, with some specimens that can weight up to 5 kg.[@courtecuisse]
\paragraph*{}
The stem height distribution also looks like a bell curve with a long right tail, a main mode around 5 cm, and a secondary mode at 0 cm. Again, this can look surprising, but some mushrooms have no stem, which could explain this height value.
\paragraph*{}
The stem width distribution looks like a bell curve with a long right tail, and a peak around 10-15 mm. In all three distributions, the right tail can probably be explained by the impossibility to have negative dimensional values.

\subsection{Single and dual criteria classifiers}  
\paragraph*{}
All further classification strategies will be elaborated, tuned and selected on one rule : the specificity on the edibility criterion must be equal to 1, false negatives are not tolerated. In layman's terms, a good algorithm may reject edible mushrooms but should **never** classify a poisonous one as edible.
\paragraph*{}
The first models one can build are simple classifiers that categorize the mushrooms as edible or poisonous, based on one or several criteria.
\paragraph*{}
Before training, validation and tuning of the algorithms, a training and validation strategy must be defined. Since these classifiers are designed from scratch, and in order to keep the code simple, a basic train-validation split was chosen, with a 90/10 split ratio between the training and validation sets.
\paragraph*{}
Once the original dataset is split in training, validation and evaluation sets, one can have a look at the characteristics of the edible and toxic populations and draw conclusions without any risk of overfitting.  

```{r, echo = FALSE, fig.height = 5.5, fig.cap = "Distribution of criteria of interest in the training set (log Y scale)"}
plot(ggarrange(
   ncol = 3, nrow = 2, legend = "bottom", common.legend = TRUE,
   train_distrib_cap.diameter + ggtitle("Cap Diameter") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   train_distrib_stem.height + ggtitle("Stem Height") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   train_distrib_stem.width + ggtitle("Stem Width") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL),
   train_distrib_habitat + ggtitle("Habitat") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL) + theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust=1)),
   train_distrib_spore.print.color + ggtitle("Spore Print Color") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL) + theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust=1)),
   train_distrib_veil.color + ggtitle("Veil Color") + ggtitle("") + ylab("") + scale_y_log10(labels = NULL) + theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust = 1))
   )
)
```  
\paragraph*{}
Differences in the distribution of edible and non edible species can already be seen in the distribution plots, and these variations will be used to classify species.
\paragraph*{}
Given the specificity constraint mentioned previously, a "stupid" algorithm can be designed as a benchmark. This algorithm will simply classify all mushrooms as poisonous, by setting all edibility values to FALSE.
\paragraph*{}
A slightly more elaborated algorithm can classify mushrooms as edible if they meet one criterion. For example, in figure 3, the mushroom should be classified as edible if *cap.diameter* > 40, if *spore.print.color* = gray or if *veil.color* = yellow.
\paragraph*{}
A more sophisticated algorithm can be elaborated with two factors (fig. 4) and will for example, classify a mushroom as edible if *cap.shape* = convex & *cap.surface* = leathery.

```{r, echo = FALSE, fig.height = 7, fig.cap = "Two-criteria plots for 5 criteria of interest"}
pair_plots
```  

\newpage
The building of this more performant model will be discussed here, since the single-criterion one is very similar and can be seen as a simplified version of the two-criteria model.
\paragraph*{}
The very first step was to build the criteria list. All unique *factor* and *logical*  types variables were gathered (using *level* value). The same operation was performed on *numeric* type values, each of them being then duplicated : one row for for *min* and one row for *max* levels. The factor/logical and the numeric datalists were then merged, and joined with a datalist that contained all *type* characteristics of each variable.
\paragraph*{}
The building of a comprehensive -- but not redundant -- 2-criteria list from this single criterion list is a four step process :

1. Create a 2-index list with the *ncomb* function, which eliminates potential duplicates because of the commutative properties of the AND operator, i.e. (n,p) indices are equivalent to (p,n) ;
2. Take each column to build the corresponding half-sized dataset, then merge the two halves ;
3. Remove all single-criterion combinations, i.e. all (n,n) indices, that are dealt with a 1-criterion model ;
4. Set all edibility criterion values to FALSE by default.
\paragraph*{}
All criteria combinations are then evaluated. The first step of this evaluation is to detect the characteristic type. All mushroom characteristics fall into one of two *lato sensu* categories : "text" (i.e. character, factor or logical) or "numbers" (i.e. integer or numeric). With a two-criteria classifier, these two types should create four combinations :

* Criterion 1 is text, criterion 2 is numeric ;
* Criterion 1 is text, criterion 2 is text ;
* Criterion 1 is numeric, criterion 2 is numeric ;
* Criterion 1 is numeric, criterion 2 is text.

However, by construction, the fourth combination does not exist, which somewhat simplifies the evaluation function. This is a consequence of the use of *ncomb* function and the placement of all numeric factors at the bottom of the criterion list. 
\paragraph*{}
After the type identification and a filtering step on the two relevant criteria, the basic classification strategy is as follows :

* If text + text : count all occurrences, count all poisonous occurrences. If there are mushrooms, but none is poisonous, consider all of them as edible.
* If text + numeric : compute min (or max, resp.) on all occurrences, and on all poisonous occurrences. If the values differ, consider all values below (or above, resp.) the poisonous occurrences to be all edible. Apply a settable safety margin.
* If numeric + numeric : compute the two min (or max, resp.) on each criteria, on all occurrences and on all poisonous occurrences. If the values differ on each dimension, consider all values below (or above, resp.) the poisonous occurrences to be all edible. Apply a settable safety margin on each dimension.

The min/max detection is very simply based on the "min" or "max" string found in *factors_listX$typeY*. A *match.fun* function converts this "min" or "max" string into a *min* or *max* function, a *recode.factor* function was then used to select the sign (+/-) of the margin to be applied. The "min" or "max" string was then concatenated to the computed numeric value, and converted into a > or < symbol, to be used in the evaluation step.
\paragraph*{}

\newpage

```{r out.width = "100%", echo = FALSE}

kable(head(relevant_factors2, 15), caption = "15 first relevant factors of the two-criteria classifier")
```

This list will serve as a basis to create a criterion list that merges all criteria two by two (for a two-criteria model) with AND operators, then merges all the pairs together with OR operators. The resulting string is simply evaluated against the validation set with a mutate function : if one criteria is met, the predicted edibility is set to TRUE, if not, it is set to FALSE.
\paragraph*{}
The predicted edibility is then compared to the real edibility value.
\paragraph*{}
By construction, the stupid classifier has *Sensitivity =* `r CM_stupid$byClass["Sensitivity"]` and *Specificity = * `r CM_stupid$byClass["Specificity"]`. The relatively good accuracy score (*Accuracy =* `r round(CM_stupid$overall["Accuracy"], 3)`) is indeed misleading, this model actually performs extremely poorly, with a F1 value that cannot be computed by the *confusionMatrix* function, since this model didn't classify correctly any of the edible mushrooms.
```{r out.width = "100%", echo = FALSE}

kable(CM_stupid$table, caption = "Confusion Matrix for stupid classifier")
```

\paragraph*{}
The single-criterion classifier performance was only marginally better than the stupid classifier's, and achieved *Sens. =* `r round(CM_monocrit$byClass["Sensitivity"], 3)` and *Spec. =* `r round(CM_monocrit$byClass["Specificity"], 3)` for a safety margin set at $m_{2} =$ `r margin1`. The F1 value was poor : *F1 =* `r round(CM_monocrit$byClass["F1"] , 3)`. Of the `r sum(CM_monocrit$table[,2])` edible specimen in the validation set, only `r sum(CM_monocrit$table[2,2])` were recognized as such.
```{r out.width = "100%", echo = FALSE}

kable(CM_monocrit$table, caption = "Confusion Matrix for single-criterion classifier")
```
\paragraph*{}
Given the poor performance of the single-criterion classifier, the performance of the dual-criteria classifier were quite surprising. With a safety margin set at $m_{2} =$ `r margin2`, this model managed to attain *Sens. =* `r round(CM_bicrit$byClass["Sensitivity"], 3)`, 
*Spec. =* `r round(CM_bicrit$byClass["Specificity"], 3)`, and 
*F1 =* `r round(CM_bicrit$byClass["F1"],3)`. Of the `r sum(CM_bicrit$table[,2])` edible specimen of the validation set, `r CM_bicrit$table[2,2]` were identified correctly.

```{r out.width = "100%", echo = FALSE}

kable(CM_bicrit$table, caption = "Confusion Matrix for dual-criteria classifier")
```
\paragraph*{}
The single-criterion and dual-criteria safety margins ($m_{1}$ and $m_{2}$) are two hyperparameters that can be tuned to give optimal results. A margin range was built, using the *seq* function, and then fed to our evaluation function through a *sapply* function.
\paragraph*{}
As said in the previous section, the endpoint is not an optimum on the ROC curve, but rather the margin that obeys to the following criteria, in decreasing priority order 

1. Specificity equal to 1,
2. Highest possible sensitivity,
3. Largest (*i.e.* safest) margin.  
\paragraph*{}
On the single-criterion model, the hyperparameter optimization gave the following results :  

```{r, echo = FALSE, fig.height = 3, fig.cap = "Sensitivity, specificity and F1-score of single-criteron classifier"}
plot(ggarrange(
   ncol = 3,
   SCtuneSensitivity,
   SCtuneSpecificity,
   SCtuneF1
   )
)
```  

An optimum was reached for $m_{1} =$ `r best_margin1["Margin"]`, which gives $Sens_{1} =$ `r best_margin1["Sensitivity"]` for $Spec_{1}$ = `r best_margin1["Specificity"]`.
\paragraph*{}
A second hyperparameter optimization run was performed on the two-criteria model, using the same $m_{1}$ value for all single-criterion classification. This run gave the following sensitivity, specificity and F1-score plots :  

```{r, echo = FALSE, fig.height = 3, fig.cap = "Sensitivity, specificity and F1-score of dual-criteron classifier"}
plot(ggarrange(
   ncol = 3,
   DCtuneSensitivity,
   DCtuneSpecificity,
   DCtuneF1
   )
)
```  

The best result was achieved with $m_{1} =$ `r best_margin1["Margin"]` and $m_{2} =$ `r best_margin2["Margin"]` , which a final performance of $Sens_{2} =$ `r best_margin2["Sensitivity"]` for $Spec_{2} =$ `r best_margin2["Specificity"]`. The specificity constraint does not provide the best F1-score.

\newpage
\subsection{Caret package models}  
The *caret* packages provides a very convenient and efficient platform for data modelling and inference. This section will explain the strategy used for the evaluation of some of these models. The selected models were of various types :

* Linear Discriminant Analysis : Linear Discriminant Analysis (lda2), Penalized Discriminant Analysis (pda)
* Generalized Additive Model : Generalized Additive Model using LOESS (gamLoess)
* Tree-Based Models : Classification And Regression Tree (CART) (rpart, rpartCost), Single C5.0 Tree (ctree)
* Random Forest : Random Ferns (rferns), Random Forest (ranger, Rborist)

\paragraph*{}
The first step was to build a regression and evaluation function. The caret package allows to define it very simply with :

```
set.seed(1)
tr_ctrl <- trainControl(classProbs = TRUE, 
                        summaryFunction = twoClassSummary, 
                        method = "cv", number = 10)
train(class ~ ., 
      method = [METHOD], 
      data = trainvalid_set, 
      trControl = tr_ctrl, 
      metric = 'Spec', 
      tuneGrid = data.frame([PARAMETERS]))
```
The *set.seed* insures reproducibility, the *trainControl* function allows to control various training and evaluation parameters ; in this case, to use a ROC/sensitivity/specificity criterion and a 10-fold cross-validation. The *train* function runs the train and evaluation process, while allowing the use of several parameters, such as the method selection the training/validation set, the metric used for the evaluation and the grid of model parameters.
\paragraph*{}
This block was included in a function to allow easy access and reproducibility. The function returns a list that can be plotted and also includes various data of interest, such as .\$results (ROC, Sensibility, Specificity), .\$bestTune (best value with the evaluation metric, Specificity in this case), and .\$finalModel (miscellanous information that can sometimes be plotted, such as trees).

\subsubsection{Linear Discriminant Analysis Models}  
The two Linear Discriminant Analysis Models selected for this study are lda2 (Linear Discriminant Analysis) and pda (Penalized Discriminant Analysis). The lda2 model has one tuning parameter (*dimen*, number of discriminant functions). The pda model also has a single tuning parameter (*lambda*, shrinkage penalty coefficient).

```{r, echo = FALSE, fig.height = 3, fig.cap = "Specificity of lda2 (left) and pda (right) models"}
plot(ggarrange(
   ncol = 2,
   fit_lda2_dim_plot + theme_bw(),
   fit_pda_lambda_plot +  ylab("") + theme_bw()
   )
)
```  
\paragraph*{}
The *dimen* parameter of the lda2 model does not seem to have much effect on specificity (*Spec =* `r round(mean(fit_lda2_dim_results[,"Spec"]), 3)`). 
\paragraph*{}
The *lambda* parameter of the sda model marginally affects its specificity, with lower lambda values giving slightly better results ($Spec_{max} =$ `r round(max(fit_pda_lambda_results["Spec"]))`).
\paragraph*{}
However, the specificity of both models are far from being sufficient for this study ($Spec \approx$ `r round(mean(c(fit_lda2_dim_results$Spec, fit_pda_lambda_results$Spec)),2)`), and their sensitivities don't seem to be much higher than the basic 2-criteria model ($Sens \approx$ `r round(mean(c(fit_lda2_dim_results$Sens, fit_pda_lambda_results$Sens)), 3)` vs `r round(best_margin2["Sensitivity"], 3)`).

\subsubsection{Generalized Additive Model}  

The only generalized additive model selected for this study is gamLoess (Generalized Additive Model using Locally Weighted Linear Regression). The caret package documentation indicates that gamLoess has two tuning parameters : *span* (fraction of data points used in the local neighborhood size) and *degree* (degree of linearization).

```{r, echo = FALSE, fig.height = 3, fig.cap = "Specificity of lda2 (left) and pda (right) models"}
plot(ggarrange(
   ncol = 2,
   fit_gamLoess_span_plot + theme_bw(),
   fit_gamLoess_degree_plot +  ylab("") + theme_bw()
   )
)
```  
\paragraph*{}
The *degree* parameter of the gamLoess model does not seem to have much effect on specificity, with $\Delta Spec =$ `r round(max(fit_gamLoess_degree_results[,"Spec"]) - min(fit_gamLoess_degree_results[,"Spec"]), 3)`). 
\paragraph*{}
The *span* parameter marginally affects the specificity of the gamLoess model, with an optimal value of *span* =`r fit_gamLoess_span_results[which.max(fit_gamLoess_span_results[,"Spec"]), "span"]`, that gives $Spec_{max} =$ `r round(max(fit_gamLoess_span_results["Spec"]), 3)` vs `r round(best_margin2["Specificity"], 4)`).
\paragraph*{}
The specificity this model does not meet the required specificity level for this study. The GamLoess model actually proved to be inferior to the 2-criteria classifier in both specificity (`r round(max(fit_gamLoess_span_results["Spec"]), 3)` vs `r best_margin2["Specificity"]`) and sensitivity (`r round(max(fit_gamLoess_span_results["Sens"]), 3)` vs `r best_margin2["Sensitivity"]`).


\subsubsection{Tree-Based Models}
\paragraph*{}
Tree-based models are of special interest in this study, for two main reasons :

* Tree-based logic is usually used in manual mushrooms classification,
* Tree models can be plotted and easily understood by humans.

\paragraph*{}
The first models presented in this study are two CART (Classification And Regression Tree) models. The basic CART model (rpart) has one complexity parameter (cp).

```{r, echo = FALSE}

kable(fit_rpart_cp_results[,1:4], digits = 5, caption = "Performance of the CART (rpart) model")
```

The basic CART model does never achieve the required specificity. However, this model still comes close and gives excellent results ($Sens_{max} =$ `r round(max(fit_rpart_cp_results["Sens"]),3)`, $Spec_{max} =$ `r round(max(fit_rpart_cp_results["Spec"]),3)`).

The second CART model used in this study (rpartCost) associates a complexity (*cp*) and a cost (*Cost*) parameters.

```{r, echo = FALSE, fig.height = 3, fig.cap = "Specificity of rpartCost according to complexity and cost values"}
plot(ggarrange(
   ncol = 2,
   fit_rpartcost_complexity_plot + scale_x_log10() + theme_bw(),
   fit_rpartcost_cost_plot + theme_bw()
   )
)
```
The best predicted specificity would be achieved with *cp =* `r fit_rpartcost_complexity_bestTune['cp']` and *Cost = * `r fit_rpartcost_cost_bestTune['Cost']`.

```{r, echo = FALSE}

kable(fit_rpartcost_best_results[,c(1:2,4:5)], digits = 5, caption = "Performance of the CART (rpartCost) model")
```
The performance, while very good, did not achieve the specificity criterion.

\paragraph*{}
The last tree-based models was the c5.0tree. This model doesn't have any tuning parameter.

```{r, echo = FALSE}
kable(fit_c50tree_results[,2:4], digits = 5, caption = "Performance of the C5.0 Tree (C5.0Tree) model")
```

Quite interestingly, despite have no tuning parameter, this model gave excellent and very balanced results out of the box, with both very high sensitivity and specificity. Still, this model didn't achieve the required *Spec* = 1 criterion.

\newpage
\subsubsection{Random Forest Models}
The Random Ferns (rFerns) model has only one parameter : depth.

```{r, echo = FALSE, fig.height = 3, fig.cap = "Specificity of Random Ferns model"}
fit_rFerns_depth_plot + theme_bw()

```
\paragraph*{}
While pretty fast, the Random Ferns Model yielded disappointing results, with a maximum specificity of only ($Spec_{max} =$ `r round(max(fit_rFerns_depth_results["Spec"]), 3)`). The maximum sensitivity wasn't very high either ($Sens_{max} =$ `r round(max(fit_rFerns_depth_results["Sens"]), 3)`). Both metrics were inferior to the basic 2-criteria model ones.
\paragraph*{}
The second random forest model is the ranger model. The caret package documentation mentions three tuning parameters : the minimal node size (*min.node.size*), the number of features to split on each node (*mtry*) and the split rule (*splitrule*).

```{r, echo = FALSE, fig.height = 2.5, fig.cap = "Ranger model specificity"}
plot(ggarrange(
   ncol = 3,
   fit_ranger_mtry_plot + scale_y_log10() + theme_bw(),
   fit_ranger_splitrule_plot + ylab("") + theme_bw(),
   fit_ranger_nodesize_plot + ylab("") + theme_bw()
   )
)
```
The preliminary tuning step showed very promising results : even with a low number of trees (*n* = 6), the required specificity was already achieved on several occurrences with single-parameter tuning. With optimal parameters (*min.node.size* = `r fit_ranger_nodesize_bestTune$min.node.size`, *mtry* = `r fit_ranger_splitrule_bestTune$mtry` and *splitrule* = `r fit_ranger_splitrule_bestTune$splitrule`).

```{r, echo = FALSE}
kable(fit_ranger_best_results[1:6], caption = "Performance of the Ranger model (optimal settings)")
```
This model achieved excellent results, giving perfect specificity and sensitivity in this evaluation phase.
\paragraph*{}
The last random forest model was provided by Rborist. The caret package mentions two tuning parameters for this mode : number of trial predictors for a split (*predFixed*) and minimum number of distinct row references to split a node (*minNode*).

```{r, echo = FALSE, fig.height = 2.5, fig.cap = "Rborist model specificity"}
plot(ggarrange(
   ncol = 2,
   fit_Rborist_pred_plot + theme_bw(),
   fit_Rborist_minNode_plot + ylab("") + theme_bw()
   )
)
```

```{r, echo = FALSE}
kable(fit_Rborist_pred_results[1:5, 1:5], digits = 4, caption = "Performance of the Rborist model (predFixed tuning)")
```


Again, the preliminary tuning step showed promising results : on this model too, the required specificity was achieved on several occurrences using only single-parameter tuning. With optimal parameters (*predFixed* = 6, manually fixed value because of the higher sensitivity), and *minNode* = `r fit_Rborist_minNode_bestTune$minNode`), the performance was estimated to to be :

```{r, echo = FALSE}
kable(fit_Rborist_best_results[1:5], caption = "Performance of the Rborist model (optimal settings)")
```

The Rborist model gave the same excellent results as the Ranger one, with perfect sensitivity and specificity.
\paragraph*{}
These results underline an interesting fact : not all random forest models are created equal. In preliminary studies, some random forest models proved to be extremely slow, some were considerably faster. This study also shows the considerable difference in sensitivity and specificity between the rFerns and the Ranger or Rborist random forest models.

\subsection{Memory Optimization}
During the building of this study, an unforeseen and unfortunate event resulted into the addition of a secondary goal : the code had to be able to handle a `r summary_number` $\times$ `r nrow(structure_dataset)` dataset and run on lower-end computers, with only 4GB RAM + 4GB swap. Memory optimization was an interesting challenge, and implied :

* Image saving on hard drive before environment cleaning,
* Identification and removal of obsolete intermediate values,
* Identification and removal of objects that won't be used in the final report,
* Identification and replacement of large objects,
* Periodic garbage collection.

The identification of large objects can be performed by the following code :

```
object_list <- objects()

obj_size <- function(fcn_object){
   object <- eval(parse(text = fcn_object))
   size <- format(object.size(object), units = "Mb")
   size <- str_remove(size, " Mb")
   size <- as.numeric(size)
   size

size_list <- sapply(X = object_list, FUN = obj_size)
```
The resulting vector can then be converted to a data frame, that returns the size of all objects :

```{r, echo = FALSE}

kable(head(sizes_list,30), caption = "Size of the 30 largest objects")
```

Some of these objects are quite large, but can fortunately be converted into smaller and still useful objects. 
\paragraph*{}
For example, the *very* large fit_rFerns_depth train object (`r sizes_list %>% filter(object == "fit_rFerns_depth") %>% select("size (Mb)") %>% round` Mb) can be split into two useful objects that gather the metrics of interest : the fitting plot (`r format(object.size(fit_rFerns_depth_plot), units = "KB")`) and the results (`r format(object.size(fit_rFerns_depth_results), units = "KB")`) table.
\paragraph*{}
Some of these objects were much smaller : ggplots are not heavy *per se*, but while generating plots for all variables proved to be useful at some point in the study, having more than forty 5+ MB plots in memory was not really necessary, especially when running training and validation steps on limited hardware.
\paragraph*{}
Periodic data-gathering and object deletion thus permitted to avoid unnecessary memory creep that resulted in major slowdowns or crashes.

\newpage
\section{Results}
\subsection{Evaluation protocol}  
\paragraph*{}
The models that attained the specificity requirement during the validation process were selected for the final evaluation. The three selected models are :

* Two-criteria classifier,
* Ranger random forest,
* Rborist random forest.

\paragraph*{}
All models were trained on the training dataset, set with the best hyperparameters values obtained by evaluating the performance against the validation dataset. Their performance against the evaluation dataset will be analyzed, using the same criteria as before :

1. Specificity *must* be equal to one.
2. Sensitivity should be the highest possible.

\subsection{Dual criteria classifier performance}  
\paragraph*{}
Running the dual criteria classifier against the evaluation dataset yields the following results :
```{r out.width = "100%", echo = FALSE}
kable(results_biclass, caption = "Performance of the bi-criteria classifier (vs. evaluation)")
```
\paragraph*{}
The confusion matrix shows more accurately the results : while `r CM_bifinal$table[2,2]` of the `r sum(CM_bifinal$table[,2])` edible specimen were accurately identified, `r CM_bifinal$table[2,1]` non-edible specimen were incorrectly classified as edible.  
```{r out.width = "100%", echo = FALSE}
kable(CM_bifinal$table, caption = "Confusion Matrix of the bi-criteria classifier (vs. evaluation)")
```  

While the model performance is quite honorable, it is not sufficient to completely fulfill the specificity criterion, which was the primary endpoint of this study. A performance decrease between the validation and evaluation stage can typically be attributed to overfitting. It is thus important to find if the performance difference is significant.
\paragraph*{}
```{r out.width = "100%", echo = FALSE}
kable(bi_perf_comp, caption = "Comparison of performance metrics of the bi-criteria classifier")
```  
The performance difference does not seem to be significant, and the lower specificity just seems to be a result of a trade-off between sensitivity and specificity. The absence of significant overfitting can also be confirmed by the slight increase of the F1 score during the validation stage.

\subsection{Random forest performance}  
The final evaluation of the Ranger gave the following confusion matrix.

```{r out.width = "100%", echo = FALSE}
kable(CM_ranger_final$table, caption = "Confusion Matrix of the Ranger model (vs. evaluation)")
```  
The final accuracy was equal to `r CM_ranger_final$overall["Accuracy"]`, with a 95% confidence interval of [`r round(CM_ranger_final$overall["AccuracyLower"], 4)`; `r round(CM_ranger_final$overall["AccuracyUpper"], 4)`]. The ranger model provided excellent results, in a very reasonable amount of time (`r time_ranger` min).
\paragraph*{}
The Rborist model gave similar results with a final accuracy equal to `r CM_Rborist_final$overall["Accuracy"]`, with a 95% confidence interval of [`r round(CM_Rborist_final$overall["AccuracyLower"], 4)`; `r round(CM_Rborist_final$overall["AccuracyUpper"], 4)`]. The Rborist model, while giving comparable results to the Ranger one, was sensibly slower (`r time_Rborist` min), although being run with a lower number of trees (*n* = 3 vs 10).


```{r out.width = "100%", echo = FALSE}
kable(rt_result, caption = "Performance of the Ranger and Rborist models (vs. evaluation)")
```  

<!--- Section that presents the modeling results and discusses the model performance. --->

\newpage
\section{Conclusion}

Species identification is a classic classification task, traditionnally performed by humans using a classification tree strategy. 
\paragraph*{}
This study showed that given particular selection conditions imposed by circumstances (toxic mushrooms shouldn't be classified as edible, i.e. *Specificity* = 1), a significant amount of modelling strategies were giving insufficient results, and proved to be inferior to even a quite basic *ad-hoc* bi-criteria classification model.
\paragraph*{}
This bi-criteria model, while already quite efficient (*Sens* = `r round(results_biclass["Sensitivity"], 3)`, *Spec* = `r round(results_biclass["Specificity"], 3)`, *F1* = `r round(results_biclass["F1"], 3)`) did not fully achieve the specificity requirement against the evaluation dataset.
\paragraph*{}
The best classification tools were random forest models : ranger and Rborist, which both gave perfect sensitivity and specificity values, but with ranger being about `r round(time_Rborist/time_ranger)` times faster. The rFerns model, while also being based on random forests, was much less accurate.
\paragraph*{}
Apart from the limitations of the starting dataset, this work has some notable limitations :

* Memory optimization could probably be improved, for example with use of local environments,
* The basic classifier code could definitely be optimized and make more use of vectorization,
* Parameter optimization was based on the caret package documentation, which doesn't mention all parameters,
* Parallelization could be an interesting strategy to make some computations much faster,
* The report didn't use dynamic references.

\paragraph*{}
This study will provide a good basis for further personal work and experimentation on these three aspects.
\paragraph*{}
Working on this dataset also gave me nice ideas about a creating a future dataset based on the same ideas as the two original datasets from Schlimmer[@mushroom1] and Wagner[@mushroom2], but with more species and more extensive criteria (such as the smell or the flesh texture) taken from more comprehensive and specialized books.[@courtecuisse]

<!--- Section that gives a brief summary of the report, its limitations and future work. --->

\newpage
\section{References}
